# -*- coding: utf-8 -*-
"""PRML_Major_Project_03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B-AyhFjMjBPJePUXJPQdSbS0XA1U576b

# **BRAIN STROKE PREDICTION**

# **PRML Major Project**

# **Loading Dataset**
"""

!pip install plotly
import plotly.express as px

import pandas as pd
df = pd.read_csv('/content/brain_stroke.csv')
df['smoking_status'].mode()[0]

"""# **Exploratory Data Analysis**"""

df.info()

df.nunique()

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Inspect the first few rows of the DataFrame
print(df.head())

# Inspect the data types of the columns
print(df.info())

# Check for missing values
print(df.isna().sum())

# Analyze the distribution of the target variable
print(df['stroke'].value_counts())

corr = df.corr()
sns.heatmap(corr, annot=True, cmap="coolwarm")

"""# **Data Visualization**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Define font parameters
font = {'family': 'serif', 'weight': 'normal', 'size': 12}

# Select columns for visualization
df_vis = df[['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'stroke']]

# Set custom color palette
sns.set_palette('hls')

# Set style to "ticks" and create count plots for each column
sns.set_style('ticks')
for col in df_vis.columns:
    plt.figure(figsize=(8, 6))
    sns.countplot(x=col, data=df_vis)
    plt.xticks(rotation=90)
    plt.title('Distribution of ' + col.capitalize(), fontdict={'fontsize': 14, 'fontweight': 'bold', 'fontfamily': 'serif'})
    plt.xlabel('')
    plt.ylabel('Number of Patients', fontdict=font)
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Define font parameters
font = {'family': 'serif', 'weight': 'normal', 'size': 12}

# Create count plot for stroke patients by gender
sns.set_palette('pastel')
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='gender', hue='stroke')
plt.title('Distribution of Stroke Patients by Gender', fontdict={'fontsize': 14, 'fontweight': 'bold', 'fontfamily': 'serif'})
plt.xlabel('')
plt.ylabel('Number of Patients', fontdict=font)
plt.legend(['No Stroke', 'Stroke'], prop={'size': 12, 'family': 'serif'})
plt.show()

# Create count plot for stroke patients by work type
sns.set_palette('pastel')
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='work_type', hue='stroke')
plt.title('Distribution of Stroke Patients by Work Type', fontdict={'fontsize': 14, 'fontweight': 'bold', 'fontfamily': 'serif'})
plt.xlabel('')
plt.ylabel('Number of Patients', fontdict=font)
plt.legend(['No Stroke', 'Stroke'], prop={'size': 12, 'family': 'serif'})
plt.show()

# Create count plot for stroke patients by smoking status
sns.set_palette('pastel')
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='smoking_status', hue='stroke')
plt.title('Distribution of Stroke Patients by Smoking Status', fontdict={'fontsize': 14, 'fontweight': 'bold', 'fontfamily': 'serif'})
plt.xlabel('')
plt.ylabel('Number of Patients', fontdict=font)
plt.legend(['No Stroke', 'Stroke'], prop={'size': 12, 'family': 'serif'})
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Define font parameters
font = {'family': 'serif', 'weight': 'normal', 'size': 12}

# Gender pie chart
gender_counts = df['gender'].value_counts()
plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', colors=sns.color_palette('pastel')[0:2],
        textprops={'fontfamily': 'serif', 'fontsize': 12})
plt.title('Distribution of Stroke Patients by Gender', fontdict={'fontsize': 14, 'fontweight': 'bold', 'fontfamily': 'serif'})
plt.show()

# Work type pie chart
work_counts = df['work_type'].value_counts()
plt.pie(work_counts, labels=work_counts.index, autopct='%1.1f%%', colors=sns.color_palette('pastel')[0:4],
        textprops={'fontfamily': 'serif', 'fontsize': 12})
plt.title('Distribution of Stroke Patients by Work Type', fontdict={'fontsize': 14, 'fontweight': 'bold', 'fontfamily': 'serif'})
plt.show()

# Smoking status pie chart
smoking_counts = df['smoking_status'].value_counts()
plt.pie(smoking_counts, labels=smoking_counts.index, autopct='%1.1f%%', colors=sns.color_palette('pastel')[0:4],
        textprops={'fontfamily': 'serif', 'fontsize': 12})
plt.title('Distribution of Stroke Patients by Smoking Status', fontdict={'fontsize': 14, 'fontweight': 'bold', 'fontfamily': 'serif'})
plt.show()

# Select numerical columns
df_num = df[['age', 'avg_glucose_level', 'bmi']]

# Define colors for each column
colors = ['orange', 'blue', 'green']

# Plot histograms for each column
for i, color in zip(df_num.columns, colors):
    plt.figure(figsize=(8, 6))
    sns.histplot(df_num[i], color=color, palette='hls', multiple='stack', shrink=0.5)
    plt.title(f'Histogram of {i.capitalize()}')
    plt.xlabel(i.capitalize())
    plt.ylabel('Count')
    plt.xticks(rotation=90)
    plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# filter the stroke patients
stroke = df[df['stroke'] == 1]

# create two separate Series for male and female stroke patients' ages
male_age = stroke[stroke["gender"] == "Male"]["age"]
female_age = stroke[stroke["gender"] == "Female"]["age"]

# plot the histogram using the Series above
plt.hist([male_age, female_age], bins=20, color=["lightblue", "pink"], alpha=0.7, label=["Male", "Female"])
plt.legend(loc="upper right")
plt.title("Stroke Ages")
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()

"""#**STEP 1: Data Preprocessing** """

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler
from imblearn.over_sampling import SMOTE

#Handle missing values
imputer = SimpleImputer(strategy='mean')
df[['bmi']] = imputer.fit_transform(df[['bmi']])

#Replace "Unknown" with mean or median in 'smoking_status' column
smoking_status_mode = df['smoking_status'].mode()[0]
df['smoking_status'].replace('Unknown', smoking_status_mode, inplace=True)

#Encode categorical variables
le = LabelEncoder()
ohe = OneHotEncoder()
df['gender'] = le.fit_transform(df['gender'])
df['ever_married'] = le.fit_transform(df['ever_married'])
df['work_type'] = ohe.fit_transform(df[['work_type']]).toarray()
df['Residence_type'] = le.fit_transform(df['Residence_type'])
df['smoking_status'] = le.fit_transform(df['smoking_status'])

#Scale numerical features
scaler = StandardScaler()
df[['age', 'avg_glucose_level']] = scaler.fit_transform(df[['age', 'avg_glucose_level']])
mmscaler = MinMaxScaler()
df[['bmi']] = mmscaler.fit_transform(df[['bmi']])

# Split the dataset into features and target variable
X = df.drop('stroke', axis=1)
y = df['stroke']

# Scale the features using StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Balancing the data using smote
s=SMOTE()
X,y=s.fit_resample(X,y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Number of samples after smote
len(y)
len(X)

"""# **STEP 2 : Feature Selection(PCA)**"""

import numpy as np
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the pipeline for PCA and classification
pca = PCA()
rfc = RandomForestClassifier()
pipe = Pipeline([('pca', pca), ('rfc', rfc)])

# Define the hyperparameters to tune
param_grid = {'pca__n_components': np.arange(1, X.shape[1]+1)}

# Perform grid search cross-validation to find the best number of components
grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)
grid.fit(X, y)

# Print the best number of components and the corresponding cross-validation score
print("Best number of components:", grid.best_params_['pca__n_components'])
print("Cross-validation score with best number of components:", grid.best_score_)

# Fit PCA on the original feature matrix X
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X)

"""#**STEP 3:Training the model**

# **Training the model Without PCA**
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Select the models
lr = LogisticRegression(random_state=42)
dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
svm = SVC(random_state=42)
mlp = MLPClassifier(random_state=42)
xgb=XGBClassifier(random_state=42)
ada=AdaBoostClassifier()
gb=GradientBoostingClassifier()
nb=GaussianNB()
knn = KNeighborsClassifier()

# Train the models
lr.fit(X_train, y_train)
dt.fit(X_train, y_train)
rf.fit(X_train, y_train)
gb.fit(X_train, y_train)
svm.fit(X_train, y_train)
mlp.fit(X_train, y_train)
xgb.fit(X_train,y_train)
ada.fit(X_train,y_train)
gb.fit(X_train,y_train)
nb.fit(X_train,y_train)
knn.fit(X_train,y_train)


# Make predictions using the best models
dt_pred = dt.predict(X_test)
rf_pred = rf.predict(X_test)
lr_pred = lr.predict(X_test)
svm_pred = svm.predict(X_test)
mlp_pred = mlp.predict(X_test)
gb_pred = gb.predict(X_test)
xgb_pred = xgb.predict(X_test)
ada_pred = ada.predict(X_test)
knn_pred = knn.predict(X_test)
nb_pred = nb.predict(X_test)

"""# **Evaluating the model without PCA**

**Accuracy Score**
"""

dt_acc = accuracy_score(y_test, dt_pred)
rf_acc = accuracy_score(y_test, rf_pred)
lr_acc = accuracy_score(y_test, lr_pred)
svm_acc = accuracy_score(y_test, svm_pred)
mlp_acc = accuracy_score(y_test, mlp_pred)
gb_acc = accuracy_score(y_test, gb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
ada_acc = accuracy_score(y_test, ada_pred)
knn_acc = accuracy_score(y_test, knn_pred)
nb_acc = accuracy_score(y_test, nb_pred)

# print the accuracy score for each model
print("Accuracy Scores: ")
print("")
print('Decision Tree:', dt_acc)
print('Random Forest:', rf_acc)
print('Logistic Regression:', lr_acc)
print('Support Vector Machine:', svm_acc)
print('Multi-Layer Perceptron:', mlp_acc)
print('Gradient Boosting:', gb_acc)
print('XGBoost:',xgb_acc)
print('AdaBoost:',ada_acc)
print('K-Nearest Neighbors:',knn_acc)
print('Naive Bayes:',nb_acc)

"""**Precision Score**"""

dt_prec = precision_score(y_test, dt_pred)
rf_prec = precision_score(y_test, rf_pred)
lr_prec = precision_score(y_test, lr_pred)
svm_prec = precision_score(y_test, svm_pred)
mlp_prec = precision_score(y_test, mlp_pred)
gb_prec = precision_score(y_test, gb_pred)
xgb_prec = precision_score(y_test, xgb_pred)
ada_prec = precision_score(y_test, ada_pred)
knn_prec = precision_score(y_test, knn_pred)
nb_prec = precision_score(y_test, nb_pred)

# print the precision score for each model
print("Precision Scores: ")
print("")
print('Decision Tree:', dt_prec)
print('Random Forest:', rf_prec)
print('Logistic Regression:', lr_prec)
print('Support Vector Machine:', svm_prec)
print('Multi-Layer Perceptron:', mlp_prec)
print('Gradient Boosting:', gb_prec)
print('XGBoost:',xgb_prec)
print('AdaBoost:',ada_prec)
print('K-Nearest Neighbors:',knn_prec)
print('Naive Bayes:',nb_prec)

""" **Recall Score**"""

dt_rec = recall_score(y_test, dt_pred)
rf_rec = recall_score(y_test, rf_pred)
lr_rec = recall_score(y_test, lr_pred)
svm_rec = recall_score(y_test, svm_pred)
mlp_rec = recall_score(y_test, mlp_pred)
gb_rec = recall_score(y_test, gb_pred)
xgb_rec = recall_score(y_test, xgb_pred)
ada_rec = recall_score(y_test, ada_pred)
knn_rec = recall_score(y_test, knn_pred)
nb_rec = recall_score(y_test, nb_pred)

# print the recall score for each model
print("Recall Scores: ")
print("")
print('Decision Tree:', dt_rec)
print('Random Forest:', rf_rec)
print('Logistic Regression:', lr_rec)
print('Support Vector Machine:', svm_rec)
print('Multi-Layer Perceptron:', mlp_rec)
print('Gradient Boosting:', gb_rec)
print('XGBoost:',xgb_rec)
print('AdaBoost:',ada_rec)
print('K-Nearest Neighbors:',knn_rec)
print('Naive Bayes:',nb_rec)

""" **F1 Score**"""

dt_f1 = f1_score(y_test, dt_pred)
rf_f1 = f1_score(y_test, rf_pred)
lr_f1 = f1_score(y_test, lr_pred)
svm_f1 = f1_score(y_test, svm_pred)
mlp_f1 = f1_score(y_test, mlp_pred)
gb_f1 = f1_score(y_test, gb_pred)
xgb_f1 = f1_score(y_test, xgb_pred)
ada_f1 = f1_score(y_test, ada_pred)
knn_f1 = f1_score(y_test, knn_pred)
nb_f1 = f1_score(y_test, nb_pred)

# print the f1 score for each model
print("F1 Scores: ")
print("")
print('Decision Tree:', dt_f1)
print('Random Forest:', rf_f1)
print('Logistic Regression:', lr_f1)
print('Support Vector Machine:', svm_f1)
print('Multi-Layer Perceptron:', mlp_f1)
print('Gradient Boosting:', gb_f1)
print('XGBoost:',xgb_f1)
print('AdaBoost:',ada_f1)
print('K-Nearest Neighbors:',knn_f1)
print('Naive Bayes:',nb_f1)

"""**ROC AUC Score**"""

dt_probs = dt.predict_proba(X_test)[:, 1]
rf_probs = rf.predict_proba(X_test)[:, 1]
lr_probs = lr.predict_proba(X_test)[:, 1]
svm_probs = svm.decision_function(X_test)
mlp_probs = mlp.predict_proba(X_test)[:, 1]
gb_probs = gb.predict_proba(X_test)[:, 1]
xgb_probs = xgb.predict_proba(X_test)[:, 1]
ada_probs = ada.predict_proba(X_test)[:, 1]
knn_probs = knn.predict_proba(X_test)[:, 1]
nb_probs = nb.predict_proba(X_test)[:, 1]

dt_auc = roc_auc_score(y_test, dt_probs)
rf_auc = roc_auc_score(y_test, rf_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
svm_auc = roc_auc_score(y_test, svm_probs)
mlp_auc = roc_auc_score(y_test, mlp_probs)
gb_auc = roc_auc_score(y_test, gb_probs)
xgb_auc = roc_auc_score(y_test, xgb_probs)
ada_auc = roc_auc_score(y_test, ada_probs)
knn_auc = roc_auc_score(y_test, knn_probs)
nb_auc = roc_auc_score(y_test, nb_probs)

# print the roc auc score for each model
print("ROC AUC Scores: ")
print("")
print('Decision Tree:', dt_auc)
print('Random Forest:', rf_auc)
print('Logistic Regression:', lr_auc)
print('Support Vector Machine:', svm_auc)
print('Multi-Layer Perceptron:', mlp_auc)
print('Gradient Boosting:', gb_auc)
print('XGBoost:',xgb_auc)
print('AdaBoost:',ada_auc)
print('K-Nearest Neighbors:',knn_auc)
print('Naive Bayes:',nb_auc)

"""**Cross Validation Score**"""

dt_scores = cross_val_score(dt, X_train, y_train, cv=5)
rf_scores = cross_val_score(rf, X_train, y_train, cv=5)
lr_scores = cross_val_score(lr, X_train, y_train, cv=5)
svm_scores = cross_val_score(svm, X_train, y_train, cv=5)
mlp_scores = cross_val_score(mlp, X_train, y_train, cv=5)
gb_scores = cross_val_score(gb, X_train, y_train, cv=5)
xgb_scores=cross_val_score(xgb,X_train,y_train,cv=5)
ada_scores=cross_val_score(ada,X_train,y_train,cv=5)
knn_scores=cross_val_score(knn,X_train,y_train,cv=5)
nb_scores=cross_val_score(nb,X_train,y_train,cv=5)

# calculate average accuracy for each model

dt_avg_acc = np.mean(dt_scores)
rf_avg_acc = np.mean(rf_scores)
lr_avg_acc = np.mean(lr_scores)
svm_avg_acc = np.mean(svm_scores)
mlp_avg_acc = np.mean(mlp_scores)
gb_avg_acc = np.mean(gb_scores)
xgb_avg_acc = np.mean(xgb_scores)
ada_avg_acc = np.mean(ada_scores)
knn_avg_acc = np.mean(knn_scores)
nb_avg_acc = np.mean(nb_scores)

# print the average accuracy for each model
print("Average Accuracy Scores: ")
print("")
print('Decision Tree:', dt_avg_acc)
print('Random Forest:', rf_avg_acc)
print('Logistic Regression:', lr_avg_acc)
print('Support Vector Machine:', svm_avg_acc)
print('Multi-Layer Perceptron:', mlp_avg_acc)
print('Gradient Boosting:', gb_avg_acc)
print('XGBoost:',xgb_avg_acc)
print('AdaBoost:',ada_avg_acc)
print('K-Nearest Neighbors:',knn_avg_acc)
print('Naive Bayes:',nb_avg_acc)
print("")

# select the best model based on its accuracy
best_model = max(lr_avg_acc, dt_avg_acc, rf_avg_acc, gb_avg_acc, svm_avg_acc, mlp_avg_acc,xgb_avg_acc,ada_avg_acc,gb_avg_acc,nb_avg_acc,knn_avg_acc)

if best_model == lr_avg_acc:
    print("The best model is Logistic Regression")
elif best_model == dt_avg_acc:
    print("The best model is Decision Tree")
elif best_model == rf_avg_acc:
    print("The best model is Random Forest")
elif best_model == gb_avg_acc:
    print("The best model is Gradient Boosting")
elif best_model == svm_avg_acc:
    print("The best model is Support Vector Machine")
elif best_model == mlp_avg_acc:
    print("The best model is Multi-Layer Perceptron")
elif best_model == xgb_avg_acc:
    print("The best model is XGBoost")
elif best_model == ada_avg_acc:
    print("The best model is AdaBoost")
elif best_model == gb_avg_acc:
    print("The best model is Gradient Boosting")
elif best_model == nb_avg_acc:
    print("The best model is Naive Bayes")
else:
    print("The best model is K-Nearest Neighbors")

"""# **STEP-3: Training the model**

## **Training the model with PCA**
"""

# Select the models
lr = LogisticRegression(random_state=42)
dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
svm = SVC(random_state=42)
mlp = MLPClassifier(random_state=42)
xgb=XGBClassifier(random_state=42)
ada=AdaBoostClassifier()
gb=GradientBoostingClassifier()
nb=GaussianNB()
knn = KNeighborsClassifier()

# Train the models on pca dataset
lr.fit(X_pca_train, y_train)
dt.fit(X_pca_train, y_train)
rf.fit(X_pca_train, y_train)
gb.fit(X_pca_train, y_train)
svm.fit(X_pca_train, y_train)
mlp.fit(X_pca_train, y_train)
xgb.fit(X_pca_train,y_train)
ada.fit(X_pca_train,y_train)
gb.fit(X_pca_train,y_train)
nb.fit(X_pca_train,y_train)
knn.fit(X_pca_train,y_train)

# Make predictions using the best models
dt_pred = dt.predict(X_pca_test)
rf_pred = rf.predict(X_pca_test)
lr_pred = lr.predict(X_pca_test)
svm_pred = svm.predict(X_pca_test)
mlp_pred = mlp.predict(X_pca_test)
gb_pred = gb.predict(X_pca_test)
xgb_pred = xgb.predict(X_pca_test)
ada_pred = ada.predict(X_pca_test)
knn_pred = knn.predict(X_pca_test)
nb_pred = nb.predict(X_pca_test)

"""## **Evaluating the model with PCA**

### **Accuracy Score**
"""

dt_acc = accuracy_score(y_test, dt_pred)
rf_acc = accuracy_score(y_test, rf_pred)
lr_acc = accuracy_score(y_test, lr_pred)
svm_acc = accuracy_score(y_test, svm_pred)
mlp_acc = accuracy_score(y_test, mlp_pred)
gb_acc = accuracy_score(y_test, gb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
ada_acc = accuracy_score(y_test, ada_pred)
knn_acc = accuracy_score(y_test, knn_pred)
nb_acc = accuracy_score(y_test, nb_pred)

# print the accuracy score for each model
print("Accuracy Scores: ")
print("")
print('Decision Tree:', dt_acc)
print('Random Forest:', rf_acc)
print('Logistic Regression:', lr_acc)
print('Support Vector Machine:', svm_acc)
print('Multi-Layer Perceptron:', mlp_acc)
print('Gradient Boosting:', gb_acc)
print('XGBoost:',xgb_acc)
print('AdaBoost:',ada_acc)
print('K-Nearest Neighbors:',knn_acc)
print('Naive Bayes:',nb_acc)

"""### **Precision Score**"""

dt_prec = precision_score(y_test, dt_pred)
rf_prec = precision_score(y_test, rf_pred)
lr_prec = precision_score(y_test, lr_pred)
svm_prec = precision_score(y_test, svm_pred)
mlp_prec = precision_score(y_test, mlp_pred)
gb_prec = precision_score(y_test, gb_pred)
xgb_prec = precision_score(y_test, xgb_pred)
ada_prec = precision_score(y_test, ada_pred)
knn_prec = precision_score(y_test, knn_pred)
nb_prec = precision_score(y_test, nb_pred)

# print the precision score for each model
print("Precision Scores: ")
print("")
print('Decision Tree:', dt_prec)
print('Random Forest:', rf_prec)
print('Logistic Regression:', lr_prec)
print('Support Vector Machine:', svm_prec)
print('Multi-Layer Perceptron:', mlp_prec)
print('Gradient Boosting:', gb_prec)
print('XGBoost:',xgb_prec)
print('AdaBoost:',ada_prec)
print('K-Nearest Neighbors:',knn_prec)
print('Naive Bayes:',nb_prec)

"""### **Recall Score**"""

dt_rec_p = recall_score(y_test, dt_pred)
rf_rec_p = recall_score(y_test, rf_pred)
lr_rec_p = recall_score(y_test, lr_pred)
svm_rec_p = recall_score(y_test, svm_pred)
mlp_rec_p = recall_score(y_test, mlp_pred)
gb_rec_p = recall_score(y_test, gb_pred)
xgb_rec_p = recall_score(y_test, xgb_pred)
ada_rec_p = recall_score(y_test, ada_pred)
knn_rec_p = recall_score(y_test, knn_pred)
nb_rec_p = recall_score(y_test, nb_pred)

# print the recall score for each model
print("Recall Scores: ")
print("")
print('Decision Tree:', dt_rec_p)
print('Random Forest:', rf_rec_p)
print('Logistic Regression:', lr_rec_p)
print('Support Vector Machine:', svm_rec_p)
print('Multi-Layer Perceptron:', mlp_rec_p)
print('Gradient Boosting:', gb_rec_p)
print('XGBoost:',xgb_rec_p)
print('AdaBoost:',ada_rec_p)
print('K-Nearest Neighbors:',knn_rec_p)
print('Naive Bayes:',nb_rec_p)

"""### **F1 Score**"""

dt_f1 = f1_score(y_test, dt_pred)
rf_f1 = f1_score(y_test, rf_pred)
lr_f1 = f1_score(y_test, lr_pred)
svm_f1 = f1_score(y_test, svm_pred)
mlp_f1 = f1_score(y_test, mlp_pred)
gb_f1 = f1_score(y_test, gb_pred)
xgb_f1 = f1_score(y_test, xgb_pred)
ada_f1 = f1_score(y_test, ada_pred)
knn_f1 = f1_score(y_test, knn_pred)
nb_f1 = f1_score(y_test, nb_pred)

# print the f1 score for each model
print("F1 Scores: ")
print("")
print('Decision Tree:', dt_f1)
print('Random Forest:', rf_f1)
print('Logistic Regression:', lr_f1)
print('Support Vector Machine:', svm_f1)
print('Multi-Layer Perceptron:', mlp_f1)
print('Gradient Boosting:', gb_f1)
print('XGBoost:',xgb_f1)
print('AdaBoost:',ada_f1)
print('K-Nearest Neighbors:',knn_f1)
print('Naive Bayes:',nb_f1)

"""### **ROC AUC Score**"""

dt_probs = dt.predict_proba(X_test)[:, 1]
rf_probs = rf.predict_proba(X_test)[:, 1]
lr_probs = lr.predict_proba(X_test)[:, 1]
svm_probs = svm.decision_function(X_test)
mlp_probs = mlp.predict_proba(X_test)[:, 1]
gb_probs = gb.predict_proba(X_test)[:, 1]
xgb_probs = xgb.predict_proba(X_test)[:, 1]
ada_probs = ada.predict_proba(X_test)[:, 1]
knn_probs = knn.predict_proba(X_test)[:, 1]
nb_probs = nb.predict_proba(X_test)[:, 1]

dt_auc = roc_auc_score(y_test, dt_probs)
rf_auc = roc_auc_score(y_test, rf_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
svm_auc = roc_auc_score(y_test, svm_probs)
mlp_auc = roc_auc_score(y_test, mlp_probs)
gb_auc = roc_auc_score(y_test, gb_probs)
xgb_auc = roc_auc_score(y_test, xgb_probs)
ada_auc = roc_auc_score(y_test, ada_probs)
knn_auc = roc_auc_score(y_test, knn_probs)
nb_auc = roc_auc_score(y_test, nb_probs)

# print the roc auc score for each model
print("ROC AUC Scores: ")
print("")
print('Decision Tree:', dt_auc)
print('Random Forest:', rf_auc)
print('Logistic Regression:', lr_auc)
print('Support Vector Machine:', svm_auc)
print('Multi-Layer Perceptron:', mlp_auc)
print('Gradient Boosting:', gb_auc)
print('XGBoost:',xgb_auc)
print('AdaBoost:',ada_auc)
print('K-Nearest Neighbors:',knn_auc)
print('Naive Bayes:',nb_auc)

"""### **Cross Validation Score**"""

dt_scores_p = cross_val_score(dt, X_pca_train, y_train, cv=5)
rf_scores_p = cross_val_score(rf, X_pca_train, y_train, cv=5)
lr_scores_p = cross_val_score(lr, X_pca_train, y_train, cv=5)
svm_scores_p = cross_val_score(svm, X_pca_train, y_train, cv=5)
mlp_scores_p = cross_val_score(mlp, X_pca_train, y_train, cv=5)
gb_scores_p=cross_val_score(gb,X_pca_train,y_train,cv=5)
xgb_scores_p=cross_val_score(xgb,X_pca_train,y_train,cv=5)
ada_scores_p=cross_val_score(ada,X_pca_train,y_train,cv=5)
knn_scores_p=cross_val_score(knn,X_pca_train,y_train,cv=5)
nb_scores_p=cross_val_score(nb,X_pca_train,y_train,cv=5)


# print the average accuracy of each model after pca
lr_avg_acc_p = np.mean(lr_scores_p)
dt_avg_acc_p = np.mean(dt_scores_p)
rf_avg_acc_p = np.mean(rf_scores_p)
gb_avg_acc_p = np.mean(gb_scores_p)
svm_avg_acc_p = np.mean(svm_scores_p)
mlp_avg_acc_p = np.mean(mlp_scores_p)
xgb_avg_acc_p = np.mean(xgb_scores_p)
ada_avg_acc_p = np.mean(ada_scores_p)
nb_avg_acc_p = np.mean(nb_scores_p)
knn_avg_acc_p = np.mean(knn_scores_p)

# print the average accuracy for each model after PCA
print(" ")
print(" ")
print("Average Accuracy Scores after PCA: ")
print(" ")
print(" ")
print('Logistic Regression:', lr_avg_acc_p)
print('Decision Tree:', dt_avg_acc_p)
print('Random Forest:', rf_avg_acc_p)
print('Gradient Boosting:', gb_avg_acc_p)
print('Support Vector Machine:', svm_avg_acc_p)
print('Multi-Layer Perceptron:', mlp_avg_acc_p)
print('XGBoost:',xgb_avg_acc_p)
print('AdaBoost:',ada_avg_acc_p)
print('Gradient Boosting:',gb_avg_acc_p)
print('Naive Bayes:',nb_avg_acc_p)
print('K-Nearest Neighbors:',knn_avg_acc_p)


# select the best model based on its accuracy
best_model = max(lr_avg_acc_p, dt_avg_acc_p, rf_avg_acc_p, gb_avg_acc_p, svm_avg_acc_p, mlp_avg_acc_p,xgb_avg_acc_p,ada_avg_acc_p,gb_avg_acc_p,nb_avg_acc_p,knn_avg_acc_p)

if best_model == lr_avg_acc_p:
    print("The best model is Logistic Regression")
elif best_model == dt_avg_acc_p:
    print("The best model is Decision Tree")
elif best_model == rf_avg_acc_p:
    print("The best model is Random Forest")
elif best_model == gb_avg_acc_p:
    print("The best model is Gradient Boosting")
elif best_model == svm_avg_acc_p:
    print("The best model is Support Vector Machine")
elif best_model == mlp_avg_acc_p:
    print("The best model is Multi-Layer Perceptron")
elif best_model == xgb_avg_acc_p:
    print("The best model is XGBoost")
elif best_model == ada_avg_acc_p:
    print("The best model is AdaBoost")
elif best_model == gb_avg_acc_p:
    print("The best model is Gradient Boosting")
elif best_model == nb_avg_acc_p:
    print("The best model is Naive Bayes")
else:
    print("The best model is K-Nearest Neighbors")

"""# **Comparing the preprocessed dataset  with and without PCA by visualizations**"""

import matplotlib.pyplot as plt

# create figure and axis objects
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# plot recall scores for each model on first axis
models = ["Logistic Regression", "Decision Tree", "Random Forest", "Gradient Boosting", 
          "Support Vector Machine", "Multi-Layer Perceptron", "XGBoost", "AdaBoost", 
          "Naive Bayes", "K-Nearest Neighbors"]
scores = [lr_rec, dt_rec, rf_rec, gb_rec,
          svm_rec, mlp_rec, xgb_rec, ada_rec,
          nb_rec, knn_rec]

ax1.bar(models, scores, color='moccasin')
ax1.set_xticklabels(models, rotation=90)
ax1.set_title("Recall Scores")
ax1.set_xlabel("Model")
ax1.set_ylabel("Recall Score")
ax1.set_ylim(0.7, 1.0)  # set y-axis limits

# plot recall scores for each model after PCA on second axis
pca_scores = [lr_rec_p, dt_rec_p, rf_rec_p, gb_rec_p,
              svm_rec_p, mlp_rec_p, xgb_rec_p, ada_rec_p,
              nb_rec_p, knn_rec_p]
ax2.bar(models, pca_scores, color='turquoise')
ax2.set_xticklabels(models, rotation=90)
ax2.set_title("Recall Scores After PCA")
ax2.set_xlabel("Model")
ax2.set_ylabel("Recall Score")
ax2.set_ylim(0.7, 1.0)  # set y-axis limits

# set overall title
fig.suptitle("Model Comparison", fontsize=16)

# display the plot
plt.show()

import matplotlib.pyplot as plt

# create figure and axis objects
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# plot cross validation scores for each model on first axis
models = ["Logistic Regression", "Decision Tree", "Random Forest", "Gradient Boosting", 
          "Support Vector Machine", "Multi-Layer Perceptron", "XGBoost", "AdaBoost", 
          "Naive Bayes", "K-Nearest Neighbors"]
scores = [lr_scores.mean(), dt_scores.mean(), rf_scores.mean(), gb_scores.mean(),
          svm_scores.mean(), mlp_scores.mean(), xgb_scores.mean(), ada_scores.mean(),
          nb_scores.mean(), knn_scores.mean()]

ax1.bar(models, scores, color='moccasin')
ax1.set_xticklabels(models, rotation=90)
ax1.set_title("Average Cross Validation Scores")
ax1.set_xlabel("Model")
ax1.set_ylabel("Average Cross Validation Score")
ax1.set_ylim(0.7, 1.0)  # set y-axis limits

# plot cross validation scores for each model after PCA on second axis
pca_scores = [lr_scores_p.mean(), dt_scores_p.mean(), rf_scores_p.mean(), gb_scores_p.mean(),
              svm_scores_p.mean(), mlp_scores_p.mean(), xgb_scores_p.mean(), ada_scores_p.mean(),
              nb_scores_p.mean(), knn_scores_p.mean()]
ax2.bar(models, pca_scores, color='turquoise')
ax2.set_xticklabels(models, rotation=90)
ax2.set_title("Average Cross Validation Scores After PCA")
ax2.set_xlabel("Model")
ax2.set_ylabel("Average Cross Validation Score")
ax2.set_ylim(0.7, 1.0)  # set y-axis limits

# set overall title
fig.suptitle("Model Comparison", fontsize=16)

# display the plot
plt.show()

"""# **STEP 4: Hyper Parameter tuning**

## **Training the model without PCA**
"""

# Select the models
lr = LogisticRegression(random_state=42)
dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
svm = SVC(random_state=42)
mlp = MLPClassifier(random_state=42)
xgb=XGBClassifier(random_state=42)
ada=AdaBoostClassifier()
gb=GradientBoostingClassifier()
nb=GaussianNB()
knn = KNeighborsClassifier()

# Define the hyperparameters to search
dt_params = {'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]}
rf_params = {'n_estimators': [100, 200, 500], 'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]}
lr_params = {'C': [0.1, 1, 10]}
svm_params = {'C': [0.01, 0.1, 1, 10, 100], 'gamma': [0.01, 0.1, 1, 10, 100]}
mlp_params = {'hidden_layer_sizes': [(100,), (50,50), (20,20,20)], 'activation': ['relu', 'tanh'], 'solver': ['adam', 'sgd'], 'alpha': [0.0001, 0.001, 0.01]}
gb_params = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10], 'learning_rate': [0.01, 0.1, 1]}
xgb_params = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10], 'learning_rate': [0.01, 0.1, 1]}
ada_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}
knn_params = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree']}
nb_params = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, None]}

# Perform grid search for each model
dt_grid = GridSearchCV(dt, dt_params, cv=5, scoring='recall')
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='recall')
lr_grid = GridSearchCV(lr, lr_params, cv=5, scoring='recall')
svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='recall')
mlp_grid = RandomizedSearchCV(mlp, mlp_params, cv=5, scoring='recall', n_iter=10)
gb_rand = RandomizedSearchCV(gb, gb_params, cv=5, scoring='recall')
xgb_rand = RandomizedSearchCV(xgb, xgb_params, cv=5, scoring='recall')
ada_rand = RandomizedSearchCV(ada, ada_params, cv=5, scoring='recall')
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='recall')
nb_grid = GridSearchCV(nb, nb_params, cv=5, scoring='recall')


# Fit the models with the best hyperparameters
dt_grid.fit(X_train, y_train)
rf_grid.fit(X_train, y_train)
lr_grid.fit(X_train, y_train)
svm_grid.fit(X_train, y_train)
mlp_grid.fit(X_train, y_train)
gb_rand.fit(X_train, y_train)
xgb_rand.fit(X_train, y_train)
ada_rand.fit(X_train, y_train)
knn_grid.fit(X_train, y_train)
nb_grid.fit(X_train, y_train)


dt_best = dt_grid.best_estimator_
rf_best = rf_grid.best_estimator_
lr_best = lr_grid.best_estimator_
svm_best = svm_grid.best_estimator_
mlp_best = mlp_grid.best_estimator_
gb_best = gb_rand.best_estimator_
xgb_best = xgb_rand.best_estimator_
ada_best = ada_rand.best_estimator_
knn_best = knn_grid.best_estimator_
nb_best = nb_grid.best_estimator_

# Make predictions using the best models
dt_pred = dt_best.predict(X_test)
rf_pred = rf_best.predict(X_test)
lr_pred = lr_best.predict(X_test)
svm_pred = svm_best.predict(X_test)
mlp_pred = mlp_best.predict(X_test)
gb_pred = gb_best.predict(X_test)
xgb_pred = xgb_best.predict(X_test)
ada_pred = ada_best.predict(X_test)
knn_pred = knn_best.predict(X_test)
nb_pred = nb_best.predict(X_test)

"""## **Evaulating the model witout PCA**

### **Accuracy Score**
"""

# Evaluate the best models
dt_acc = accuracy_score(y_test, dt_pred)
rf_acc = accuracy_score(y_test, rf_pred)
lr_acc = accuracy_score(y_test, lr_pred)
svm_acc = accuracy_score(y_test, svm_pred)
mlp_acc = accuracy_score(y_test, mlp_pred)
gb_acc = accuracy_score(y_test, gb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
ada_acc = accuracy_score(y_test, ada_pred)
knn_acc = accuracy_score(y_test, knn_pred)
nb_acc = accuracy_score(y_test, nb_pred)

# print the accuracy score for each model
print("Accuracy Scores: ")
print("")
print('Decision Tree:', dt_acc)
print('Random Forest:', rf_acc)
print('Logistic Regression:', lr_acc)
print('Support Vector Machine:', svm_acc)
print('Multi-Layer Perceptron:', mlp_acc)
print('Gradient Boosting:', gb_acc)
print('XGBoost:',xgb_acc)
print('AdaBoost:',ada_acc)
print('K-Nearest Neighbors:',knn_acc)
print('Naive Bayes:',nb_acc)

"""### **Precision Score**"""

dt_prec = precision_score(y_test, dt_pred)
rf_prec = precision_score(y_test, rf_pred)
lr_prec = precision_score(y_test, lr_pred)
svm_prec = precision_score(y_test, svm_pred)
mlp_prec = precision_score(y_test, mlp_pred)
gb_prec = precision_score(y_test, gb_pred)
xgb_prec = precision_score(y_test, xgb_pred)
ada_prec = precision_score(y_test, ada_pred)
knn_prec = precision_score(y_test, knn_pred)
nb_prec = precision_score(y_test, nb_pred)

# print the precision score for each model
print("Precision Scores: ")
print("")
print('Decision Tree:', dt_prec)
print('Random Forest:', rf_prec)
print('Logistic Regression:', lr_prec)
print('Support Vector Machine:', svm_prec)
print('Multi-Layer Perceptron:', mlp_prec)
print('Gradient Boosting:', gb_prec)
print('XGBoost:',xgb_prec)
print('AdaBoost:',ada_prec)
print('K-Nearest Neighbors:',knn_prec)
print('Naive Bayes:',nb_prec)

"""### **Recall Score**"""

# Print the evaluation resultsdt_rec = recall_score(y_test, dt_pred)
rf_rec = recall_score(y_test, rf_pred)
lr_rec = recall_score(y_test, lr_pred)
svm_rec = recall_score(y_test, svm_pred)
mlp_rec = recall_score(y_test, mlp_pred)
gb_rec = recall_score(y_test, gb_pred)
xgb_rec = recall_score(y_test, xgb_pred)
ada_rec = recall_score(y_test, ada_pred)
knn_rec = recall_score(y_test, knn_pred)
nb_rec = recall_score(y_test, nb_pred)

# print the recall score for each model
print("Recall Scores: ")
print("")
print('Decision Tree:', dt_rec)
print('Random Forest:', rf_rec)
print('Logistic Regression:', lr_rec)
print('Support Vector Machine:', svm_rec)
print('Multi-Layer Perceptron:', mlp_rec)
print('Gradient Boosting:', gb_rec)
print('XGBoost:',xgb_rec)
print('AdaBoost:',ada_rec)
print('K-Nearest Neighbors:',knn_rec)
print('Naive Bayes:',nb_rec)

"""### **F1 Score**"""

dt_f1 = f1_score(y_test, dt_pred)
rf_f1 = f1_score(y_test, rf_pred)
lr_f1 = f1_score(y_test, lr_pred)
svm_f1 = f1_score(y_test, svm_pred)
mlp_f1 = f1_score(y_test, mlp_pred)
gb_f1 = f1_score(y_test, gb_pred)
xgb_f1 = f1_score(y_test, xgb_pred)
ada_f1 = f1_score(y_test, ada_pred)
knn_f1 = f1_score(y_test, knn_pred)
nb_f1 = f1_score(y_test, nb_pred)

# print the f1 score for each model
print("F1 Scores: ")
print("")
print('Decision Tree:', dt_f1)
print('Random Forest:', rf_f1)
print('Logistic Regression:', lr_f1)
print('Support Vector Machine:', svm_f1)
print('Multi-Layer Perceptron:', mlp_f1)
print('Gradient Boosting:', gb_f1)
print('XGBoost:',xgb_f1)
print('AdaBoost:',ada_f1)
print('K-Nearest Neighbors:',knn_f1)
print('Naive Bayes:',nb_f1)

"""### **ROC AUC Score**"""

dt_probs = dt_best.predict_proba(X_test)[:, 1]
rf_probs = rf_best.predict_proba(X_test)[:, 1]
lr_probs = lr_best.predict_proba(X_test)[:, 1]
svm_probs = svm_best.decision_function(X_test)
mlp_probs = mlp_best.predict_proba(X_test)[:, 1]
gb_probs = gb_best.predict_proba(X_test)[:, 1]
xgb_probs = xgb_best.predict_proba(X_test)[:, 1]
ada_probs = ada_best.predict_proba(X_test)[:, 1]
knn_probs = knn_best.predict_proba(X_test)[:, 1]
nb_probs = nb_best.predict_proba(X_test)[:, 1]

dt_auc = roc_auc_score(y_test, dt_probs)
rf_auc = roc_auc_score(y_test, rf_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
svm_auc = roc_auc_score(y_test, svm_probs)
mlp_auc = roc_auc_score(y_test, mlp_probs)
gb_auc = roc_auc_score(y_test, gb_probs)
xgb_auc = roc_auc_score(y_test, xgb_probs)
ada_auc = roc_auc_score(y_test, ada_probs)
knn_auc = roc_auc_score(y_test, knn_probs)
nb_auc = roc_auc_score(y_test, nb_probs)

# print the roc auc score for each model
print("ROC AUC Scores: ")
print("")
print('Decision Tree:', dt_auc)
print('Random Forest:', rf_auc)
print('Logistic Regression:', lr_auc)
print('Support Vector Machine:', svm_auc)
print('Multi-Layer Perceptron:', mlp_auc)
print('Gradient Boosting:', gb_auc)
print('XGBoost:',xgb_auc)
print('AdaBoost:',ada_auc)
print('K-Nearest Neighbors:',knn_auc)
print('Naive Bayes:',nb_auc)

"""## **Training the model With PCA**"""

# Select the models
lr = LogisticRegression(random_state=42)
dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
svm = SVC(random_state=42)
mlp = MLPClassifier(random_state=42)
xgb=XGBClassifier(random_state=42)
ada=AdaBoostClassifier()
gb=GradientBoostingClassifier()
nb=GaussianNB()
knn = KNeighborsClassifier()

# Define the hyperparameters to search
dt_params = {'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]}
rf_params = {'n_estimators': [100, 200, 500], 'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]}
lr_params = {'C': [0.1, 1, 10]}
svm_params = {'C': [0.01, 0.1, 1, 10, 100], 'gamma': [0.01, 0.1, 1, 10, 100]}
mlp_params = {'hidden_layer_sizes': [(100,), (50,50), (20,20,20)], 'activation': ['relu', 'tanh'], 'solver': ['adam', 'sgd'], 'alpha': [0.0001, 0.001, 0.01]}
gb_params = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10], 'learning_rate': [0.01, 0.1, 1]}
xgb_params = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10], 'learning_rate': [0.01, 0.1, 1]}
ada_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}
knn_params = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree']}
nb_params = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, None]}

# Perform grid search for each model
dt_grid = GridSearchCV(dt, dt_params, cv=5, scoring='recall')
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='recall')
lr_grid = GridSearchCV(lr, lr_params, cv=5, scoring='recall')
svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='recall')
mlp_grid = RandomizedSearchCV(mlp, mlp_params, cv=5, scoring='recall', n_iter=10)
gb_rand = RandomizedSearchCV(gb, gb_params, cv=5, scoring='recall')
xgb_rand = RandomizedSearchCV(xgb, xgb_params, cv=5, scoring='recall')
ada_rand = RandomizedSearchCV(ada, ada_params, cv=5, scoring='recall')
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='recall')
nb_grid = GridSearchCV(nb, nb_params, cv=5, scoring='recall')


# Fit the models with the best hyperparameters
dt_grid.fit(X_pca_train, y_train)
rf_grid.fit(X_pca_train, y_train)
lr_grid.fit(X_pca_train, y_train)
svm_grid.fit(X_pca_train, y_train)
mlp_grid.fit(X_pca_train, y_train)
gb_rand.fit(X_pca_train, y_train)
xgb_rand.fit(X_pca_train, y_train)
ada_rand.fit(X_pca_train, y_train)
knn_grid.fit(X_pca_train, y_train)
nb_grid.fit(X_pca_train, y_train)



dt_best = dt_grid.best_estimator_
rf_best = rf_grid.best_estimator_
lr_best = lr_grid.best_estimator_
svm_best = svm_grid.best_estimator_
mlp_best = mlp_grid.best_estimator_
gb_best = gb_rand.best_estimator_
xgb_best = xgb_rand.best_estimator_
ada_best = ada_rand.best_estimator_
knn_best = knn_grid.best_estimator_
nb_best = nb_grid.best_estimator_

# Make predictions using the best models
dt_pred = dt_best.predict(X_pca_test)
rf_pred = rf_best.predict(X_pca_test)
lr_pred = lr_best.predict(X_pca_test)
svm_pred = svm_best.predict(X_pca_test)
mlp_pred = mlp_best.predict(X_pca_test)
gb_pred = gb_best.predict(X_pca_test)
xgb_pred = xgb_best.predict(X_pca_test)
ada_pred = ada_best.predict(X_pca_test)
knn_pred = knn_best.predict(X_pca_test)
nb_pred = nb_best.predict(X_pca_test)

# Evaluate the best models
dt_acc = accuracy_score(y_test, dt_pred)
rf_acc = accuracy_score(y_test, rf_pred)
lr_acc = accuracy_score(y_test, lr_pred)
svm_acc = accuracy_score(y_test, svm_pred)
mlp_acc = accuracy_score(y_test, mlp_pred)
gb_acc = accuracy_score(y_test, gb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
ada_acc = accuracy_score(y_test, ada_pred)
knn_acc = accuracy_score(y_test, knn_pred)
nb_acc = accuracy_score(y_test, nb_pred)



dt_prec = precision_score(y_test, dt_pred)
rf_prec = precision_score(y_test, rf_pred)
lr_prec = precision_score(y_test, lr_pred)
svm_prec = precision_score(y_test, svm_pred)
mlp_prec = precision_score(y_test, mlp_pred)
gb_prec = precision_score(y_test, gb_pred)
xgb_prec = precision_score(y_test, xgb_pred)
ada_prec = precision_score(y_test, ada_pred)
knn_prec = precision_score(y_test, knn_pred)
nb_prec = precision_score(y_test, nb_pred)


# Print the evaluation resultsdt_rec = recall_score(y_test, dt_pred)
rf_rec = recall_score(y_test, rf_pred)
lr_rec = recall_score(y_test, lr_pred)
svm_rec = recall_score(y_test, svm_pred)
mlp_rec = recall_score(y_test, mlp_pred)
gb_rec = recall_score(y_test, gb_pred)
xgb_rec = recall_score(y_test, xgb_pred)
ada_rec = recall_score(y_test, ada_pred)
knn_rec = recall_score(y_test, knn_pred)
nb_rec = recall_score(y_test, nb_pred)


dt_f1 = f1_score(y_test, dt_pred)
rf_f1 = f1_score(y_test, rf_pred)
lr_f1 = f1_score(y_test, lr_pred)
svm_f1 = f1_score(y_test, svm_pred)
mlp_f1 = f1_score(y_test, mlp_pred)
gb_f1 = f1_score(y_test, gb_pred)
xgb_f1 = f1_score(y_test, xgb_pred)
ada_f1 = f1_score(y_test, ada_pred)
knn_f1 = f1_score(y_test, knn_pred)
nb_f1 = f1_score(y_test, nb_pred)


dt_probs = dt_best.predict_proba(X_pca_test)[:, 1]
rf_probs = rf_best.predict_proba(X_pca_test)[:, 1]
lr_probs = lr_best.predict_proba(X_pca_test)[:, 1]
svm_probs = svm_best.decision_function(X_pca_test)
mlp_probs = mlp_best.predict_proba(X_pca_test)[:, 1]
gb_probs = gb_best.predict_proba(X_pca_test)[:, 1]
xgb_probs = xgb_best.predict_proba(X_pca_test)[:, 1]
ada_probs = ada_best.predict_proba(X_pca_test)[:, 1]
knn_probs = knn_best.predict_proba(X_pca_test)[:, 1]
nb_probs = nb_best.predict_proba(X_pca_test)[:, 1]


dt_auc = roc_auc_score(y_test, dt_probs)
rf_auc = roc_auc_score(y_test, rf_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
svm_auc = roc_auc_score(y_test, svm_probs)
mlp_auc = roc_auc_score(y_test, mlp_probs)
gb_auc = roc_auc_score(y_test, gb_probs)
xgb_auc = roc_auc_score(y_test, xgb_probs)
ada_auc = roc_auc_score(y_test, ada_probs)
knn_auc = roc_auc_score(y_test, knn_probs)
nb_auc = roc_auc_score(y_test, nb_probs)

"""## **Evaluating the Models with PCA**

### **Accuracy Score**
"""

# Evaluate the best models
dt_acc = accuracy_score(y_test, dt_pred)
rf_acc = accuracy_score(y_test, rf_pred)
lr_acc = accuracy_score(y_test, lr_pred)
svm_acc = accuracy_score(y_test, svm_pred)
mlp_acc = accuracy_score(y_test, mlp_pred)
gb_acc = accuracy_score(y_test, gb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
ada_acc = accuracy_score(y_test, ada_pred)
knn_acc = accuracy_score(y_test, knn_pred)
nb_acc = accuracy_score(y_test, nb_pred)

# print the accuracy score for each model
print("Accuracy Scores: ")
print("")
print('Decision Tree:', dt_acc)
print('Random Forest:', rf_acc)
print('Logistic Regression:', lr_acc)
print('Support Vector Machine:', svm_acc)
print('Multi-Layer Perceptron:', mlp_acc)
print('Gradient Boosting:', gb_acc)
print('XGBoost:',xgb_acc)
print('AdaBoost:',ada_acc)
print('K-Nearest Neighbors:',knn_acc)
print('Naive Bayes:',nb_acc)

""" Recall Score"""

dt_prec = precision_score(y_test, dt_pred)
rf_prec = precision_score(y_test, rf_pred)
lr_prec = precision_score(y_test, lr_pred)
svm_prec = precision_score(y_test, svm_pred)
mlp_prec = precision_score(y_test, mlp_pred)
gb_prec = precision_score(y_test, gb_pred)
xgb_prec = precision_score(y_test, xgb_pred)
ada_prec = precision_score(y_test, ada_pred)
knn_prec = precision_score(y_test, knn_pred)
nb_prec = precision_score(y_test, nb_pred)

# print the precision score for each model
print("Precision Scores: ")
print("")
print('Decision Tree:', dt_prec)
print('Random Forest:', rf_prec)
print('Logistic Regression:', lr_prec)
print('Support Vector Machine:', svm_prec)
print('Multi-Layer Perceptron:', mlp_prec)
print('Gradient Boosting:', gb_prec)
print('XGBoost:',xgb_prec)
print('AdaBoost:',ada_prec)
print('K-Nearest Neighbors:',knn_prec)
print('Naive Bayes:',nb_prec)

"""### **Recall Score**"""

# Print the evaluation resultsdt_rec = recall_score(y_test, dt_pred)
rf_rec = recall_score(y_test, rf_pred)
lr_rec = recall_score(y_test, lr_pred)
svm_rec = recall_score(y_test, svm_pred)
mlp_rec = recall_score(y_test, mlp_pred)
gb_rec = recall_score(y_test, gb_pred)
xgb_rec = recall_score(y_test, xgb_pred)
ada_rec = recall_score(y_test, ada_pred)
knn_rec = recall_score(y_test, knn_pred)
nb_rec = recall_score(y_test, nb_pred)

# print the recall score for each model
print("Recall Scores: ")
print("")
print('Decision Tree:', dt_rec)
print('Random Forest:', rf_rec)
print('Logistic Regression:', lr_rec)
print('Support Vector Machine:', svm_rec)
print('Multi-Layer Perceptron:', mlp_rec)
print('Gradient Boosting:', gb_rec)
print('XGBoost:',xgb_rec)
print('AdaBoost:',ada_rec)
print('K-Nearest Neighbors:',knn_rec)
print('Naive Bayes:',nb_rec)

"""### **F1 Score**"""

dt_f1 = f1_score(y_test, dt_pred)
rf_f1 = f1_score(y_test, rf_pred)
lr_f1 = f1_score(y_test, lr_pred)
svm_f1 = f1_score(y_test, svm_pred)
mlp_f1 = f1_score(y_test, mlp_pred)
gb_f1 = f1_score(y_test, gb_pred)
xgb_f1 = f1_score(y_test, xgb_pred)
ada_f1 = f1_score(y_test, ada_pred)
knn_f1 = f1_score(y_test, knn_pred)
nb_f1 = f1_score(y_test, nb_pred)

# print the f1 score for each model
print("F1 Scores: ")
print("")
print('Decision Tree:', dt_f1)
print('Random Forest:', rf_f1)
print('Logistic Regression:', lr_f1)
print('Support Vector Machine:', svm_f1)
print('Multi-Layer Perceptron:', mlp_f1)
print('Gradient Boosting:', gb_f1)
print('XGBoost:',xgb_f1)
print('AdaBoost:',ada_f1)
print('K-Nearest Neighbors:',knn_f1)
print('Naive Bayes:',nb_f1)

"""### **ROC AUC Score**"""

dt_probs = dt_best.predict_proba(X_test)[:, 1]
rf_probs = rf_best.predict_proba(X_test)[:, 1]
lr_probs = lr_best.predict_proba(X_test)[:, 1]
svm_probs = svm_best.decision_function(X_test)
mlp_probs = mlp_best.predict_proba(X_test)[:, 1]
gb_probs = gb_best.predict_proba(X_test)[:, 1]
xgb_probs = xgb_best.predict_proba(X_test)[:, 1]
ada_probs = ada_best.predict_proba(X_test)[:, 1]
knn_probs = knn_best.predict_proba(X_test)[:, 1]
nb_probs = nb_best.predict_proba(X_test)[:, 1]

dt_auc = roc_auc_score(y_test, dt_probs)
rf_auc = roc_auc_score(y_test, rf_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
svm_auc = roc_auc_score(y_test, svm_probs)
mlp_auc = roc_auc_score(y_test, mlp_probs)
gb_auc = roc_auc_score(y_test, gb_probs)
xgb_auc = roc_auc_score(y_test, xgb_probs)
ada_auc = roc_auc_score(y_test, ada_probs)
knn_auc = roc_auc_score(y_test, knn_probs)
nb_auc = roc_auc_score(y_test, nb_probs)

# print the roc auc score for each model
print("ROC AUC Scores: ")
print("")
print('Decision Tree:', dt_auc)
print('Random Forest:', rf_auc)
print('Logistic Regression:', lr_auc)
print('Support Vector Machine:', svm_auc)
print('Multi-Layer Perceptron:', mlp_auc)
print('Gradient Boosting:', gb_auc)
print('XGBoost:',xgb_auc)
print('AdaBoost:',ada_auc)
print('K-Nearest Neighbors:',knn_auc)
print('Naive Bayes:',nb_auc)

"""Using Bagging Classifier"""

from imblearn.ensemble import BalancedBaggingClassifier

#Create an instance
bbc = BalancedBaggingClassifier(estimator=rf_best,
                                sampling_strategy='not majority',
                                replacement=False,
                                random_state=42)
bbc.fit(X_train, y_train)
preds = bbc.predict(X_test)


print('recall: ', recall_score(y_test, preds))
print('precision: ', precision_score(y_test, preds))
print('f1_score: ', f1_score(y_test, preds))
print('auc: ', roc_auc_score(y_test, preds))
print('accuracy',accuracy_score(preds, y_test)*100)

from imblearn.ensemble import BalancedBaggingClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Define the hyperparameters to search
bbc_params = {'n_estimators': [50, 100, 200], 'max_samples': [0.5, 0.7, 0.9], 'max_features': [0.5, 0.7, 0.9]}

# Define the base estimator
rf_base = RandomForestClassifier()

# Define the balanced bagging classifier with the base estimator and the hyperparameters to search
bbc_rand = RandomizedSearchCV(BalancedBaggingClassifier(base_estimator=rf_base, sampling_strategy='not majority', replacement=False, random_state=42),
                              bbc_params, cv=5, scoring='roc_auc')

# Fit the balanced bagging classifier with the best hyperparameters
bbc_rand.fit(X_train, y_train)

# Make predictions using the best model
bbc_pred = bbc_rand.predict(X_test)

# Evaluate the best model
bbc_acc = accuracy_score(y_test, bbc_pred)
bbc_prec = precision_score(y_test, bbc_pred)
bbc_rec = recall_score(y_test, bbc_pred)
bbc_f1 = f1_score(y_test, bbc_pred)
bbc_probs = bbc_rand.predict_proba(X_test)[:, 1]
bbc_auc = roc_auc_score(y_test, bbc_probs)

# Print the evaluation results
print('BalancedBagging: Accuracy = {:.4f}, Precision = {:.4f}, Recall = {:.4f}, F1-score = {:.4f}, AUC = {:.4f}'.format(bbc_acc, bbc_prec, bbc_rec, bbc_f1, bbc_auc))

from imblearn.ensemble import BalancedBaggingClassifier

#Create an instance
bbc = BalancedBaggingClassifier(estimator=svm_best,
                                sampling_strategy='not majority',
                                replacement=False,
                                random_state=42)
bbc.fit(X_train, y_train)
preds = bbc.predict(X_test)


print('recall: ', recall_score(y_test, preds))
print('precision: ', precision_score(y_test, preds))
print('f1_score: ', f1_score(y_test, preds))
print('auc: ', roc_auc_score(y_test, preds))
print('accuracy',accuracy_score(preds, y_test)*100)

from imblearn.ensemble import BalancedBaggingClassifier

#Create an instance
bbc = BalancedBaggingClassifier(estimator=mlp_best,
                                sampling_strategy='not majority',
                                replacement=False,
                                random_state=42)
bbc.fit(X_train, y_train)
preds = bbc.predict(X_test)


print('recall: ', recall_score(y_test, preds))
print('precision: ', precision_score(y_test, preds))
print('f1_score: ', f1_score(y_test, preds))
print('auc: ', roc_auc_score(y_test, preds))
print('accuracy',accuracy_score(preds, y_test)*100)

from imblearn.ensemble import BalancedBaggingClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Define the hyperparameters to search
bbc_params = {'n_estimators': [50, 100, 200], 'max_samples': [0.5, 0.7, 0.9], 'max_features': [0.5, 0.7, 0.9]}

# Define the base estimator
knn_base = knn_best

# Define the balanced bagging classifier with the base estimator and the hyperparameters to search
bbc_rand = RandomizedSearchCV(BalancedBaggingClassifier(base_estimator=knn_base, sampling_strategy='not majority', replacement=False, random_state=42),
                              bbc_params, cv=5, scoring='roc_auc')

# Fit the balanced bagging classifier with the best hyperparameters
bbc_rand.fit(X_train, y_train)

# Make predictions using the best model
bbc_pred = bbc_rand.predict(X_test)

# Evaluate the best model
bbc_acc = accuracy_score(y_test, bbc_pred)
bbc_prec = precision_score(y_test, bbc_pred)
bbc_rec = recall_score(y_test, bbc_pred)
bbc_f1 = f1_score(y_test, bbc_pred)
bbc_probs = bbc_rand.predict_proba(X_test)[:, 1]
bbc_auc = roc_auc_score(y_test, bbc_probs)

# Print the evaluation results
print('BalancedBagging: Accuracy = {:.4f}, Precision = {:.4f}, Recall = {:.4f}, F1-score = {:.4f}, AUC = {:.4f}'.format(bbc_acc, bbc_prec, bbc_rec, bbc_f1, bbc_auc))

from imblearn.ensemble import BalancedBaggingClassifier

#Create an instance
bbc = BalancedBaggingClassifier(estimator=gb_best,
                                sampling_strategy='not majority',
                                replacement=False,
                                random_state=42)
bbc.fit(X_train, y_train)
preds = bbc.predict(X_test)


print('recall: ', recall_score(y_test, preds))
print('precision: ', precision_score(y_test, preds))
print('f1_score: ', f1_score(y_test, preds))
print('auc: ', roc_auc_score(y_test, preds))
print('accuracy',accuracy_score(preds, y_test)*100)

"""Best model among bagging"""

from imblearn.ensemble import BalancedBaggingClassifier

#Create an instance
bbc = BalancedBaggingClassifier(estimator=knn_best,
                                sampling_strategy='not majority',
                                replacement=False,
                                random_state=42)
bbc.fit(X_train, y_train)
preds = bbc.predict(X_test)


print('recall: ', recall_score(y_test, preds))
print('precision: ', precision_score(y_test, preds))
print('f1_score: ', f1_score(y_test, preds))
print('auc: ', roc_auc_score(y_test, preds))
print('accuracy',accuracy_score(preds, y_test)*100)